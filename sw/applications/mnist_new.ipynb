{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2022 ETH Zurich and University of Bologna.\n",
    "# Licensed under the Apache License, Version 2.0, see LICENSE for details.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import pathlib\n",
    "import hjson\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "global verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_cstr(a):\n",
    "    out = '{'\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = a.flat\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        a = a.numpy().flat\n",
    "    for el in a:\n",
    "        out += '{}, '.format(el)\n",
    "    out = out[:-2] + '}'\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check what is missing for CNN\n",
    "def emit_mnist_cnn_data(name='mnist_cnn', **kwargs):\n",
    "    \n",
    "    # constants\n",
    "    ## output channels\n",
    "    co = kwargs['CO']\n",
    "    ## input channels\n",
    "    ci = kwargs['CI']\n",
    "    ## input height \n",
    "    h = kwargs['H']\n",
    "    ## input width\n",
    "    w = kwargs['W']\n",
    "    ## filter dimension (assuming square)\n",
    "    k = kwargs['K']\n",
    "    ## padding\n",
    "    p = kwargs['padding']\n",
    "    ## stride\n",
    "    s = kwargs['stride']\n",
    "    \n",
    "    # data\n",
    "    MAT_INPUT = kwargs['INPUT']\n",
    "    MAT_CONV1_WEIGHTS = kwargs['CONV1_WEIGHTS']\n",
    "    MAT_CONV1_BIAS = kwargs['CONV1_BIAS']\n",
    "\n",
    "\n",
    "    layer_str = ''\n",
    "    layer_str += '#include \"network.h\"\\n\\n'\n",
    "    layer_str += f'cnn_t {name}_t = {{\\n'\n",
    "    layer_str += f'\\t.CO = {co},\\n'\n",
    "    layer_str += f'\\t.CI = {ci},\\n'\n",
    "    layer_str += f'\\t.H = {h},\\n'\n",
    "    layer_str += f'\\t.W = {w},\\n'\n",
    "    layer_str += f'\\t.K = {k},\\n'\n",
    "    layer_str += f'\\t.padding = {p},\\n'\n",
    "    layer_str += f'\\t.stride = {s},\\n'\n",
    "    layer_str += f'\\t.dtype = FP{kwargs[\"prec\"]}\\n'\n",
    "    layer_str += '};\\n\\n\\n'\n",
    "\n",
    "    ctypes = {\n",
    "        '64': 'double',\n",
    "        '32': 'float',\n",
    "        '16': '__fp16',\n",
    "        'B16': '__bf16',\n",
    "        '8': 'char'\n",
    "    }\n",
    "\n",
    "    dtype = ctypes[str(kwargs['prec'])]\n",
    "\n",
    "    # network initialization parameters\n",
    "    layer_str += f'static {dtype} {name}_conv1_weights_dram [{co}][{ci}][{k}][{k}] = ' + array_to_cstr(MAT_CONV1_WEIGHTS) + ';\\n\\n\\n'\n",
    "    layer_str += f'static {dtype} {name}_conv1_bias_dram [{co}] = ' + array_to_cstr(MAT_CONV1_BIAS) + ';\\n\\n\\n'\n",
    "\n",
    "    # input data\n",
    "    layer_str += f'static {dtype} {name}_image_dram [{w*h}][{1}] = ' + array_to_cstr(MAT_INPUT) + ';\\n\\n\\n'\n",
    "\n",
    "    return layer_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_mnist_header_file(layer_type: str, **kwargs):\n",
    "\n",
    "    file_path = '/scratch/msc22f11/msc22f11/snitch/sw/applications/data/'\n",
    "    emit_str = \"// Copyright 2022 ETH Zurich and University of Bologna.\\n\" + \\\n",
    "               \"// Licensed under the Apache License, Version 2.0, see LICENSE for details.\\n\" + \\\n",
    "               \"// SPDX-License-Identifier: Apache-2.0\\n\\n\"\n",
    "\n",
    "    if(layer_type == 'mnist_cnn'):\n",
    "        file = file_path + 'data_cnn_mnist.h'\n",
    "        emit_str += emit_mnist_cnn_data(**kwargs)\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(emit_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download MNIST dataset using DataLoader\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "mnist_dataset = MNIST(PATH_DATASETS, train=True, transform=transform, download=True)\n",
    "\n",
    "# set seeds for reproducability \n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "mnist_dl = DataLoader(mnist_dataset, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: .\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        torch.manual_seed(42)\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        torch.manual_seed(42)\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print(x.shape)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(x.shape)       \n",
    "        output = self.out(x)\n",
    "        print(output)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CNN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we iterate through a smaller subset of the dataset \n",
    "to retrieve the image data with their\n",
    "respective labels\n",
    "\"\"\"\n",
    "\n",
    "data_iterator = iter(mnist_dl)\n",
    "\n",
    "for i in range(0, 5):\n",
    "    s_image, s_label = data_iterator.next()\n",
    "    np_s_image = s_image.numpy().flatten()\n",
    "    np_s_label = s_label.numpy().flatten()\n",
    "    if(i==0):\n",
    "        s_images = np.array(np_s_image.tolist())\n",
    "        s_labels = np.array(np_s_label.tolist())\n",
    "    else:\n",
    "        s_images = np.append(s_images, np_s_image)\n",
    "        s_labels = np.append(s_labels, np_s_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_im, first_label = next(iter(mnist_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of CONV1 weights:  torch.Size([16, 1, 5, 5])\n",
      "Shape of CONV1 biases:  torch.Size([16])\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 16, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1568])\n",
      "tensor([[-0.1005,  0.0660, -0.0802,  0.0697, -0.0005,  0.0110, -0.0027, -0.0046,\n",
      "          0.0114,  0.0355]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "weights_conv1 = net.conv1[0].weight\n",
    "print('Shape of CONV1 weights: ', weights_conv1.shape)\n",
    "biases_conv1 = net.conv1[0].bias\n",
    "print('Shape of CONV1 biases: ', biases_conv1.shape)\n",
    "weights_conv2 = net.conv2[0].weight\n",
    "biases_conv2 = net.conv2[0].bias\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(1):\n",
    "    net.zero_grad\n",
    "    output = net(first_im)[0]\n",
    "    loss = criterion(output, first_label)\n",
    "    loss.backward()\n",
    "    weight_grads_conv1 = net.conv1[0].weight.grad\n",
    "    bias_grads_conv1 = net.conv1[0].bias.grad\n",
    "    weight_grads_conv2 = net.conv2[0].weight.grad\n",
    "    bias_grads_conv2 = net.conv2[0].bias.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'CO': 16,\n",
    "    'CI': 1,\n",
    "    'H': 28,\n",
    "    'W': 28,\n",
    "    'K': 5,\n",
    "    'padding': 2,\n",
    "    'stride': 1,\n",
    "    'INPUT': first_im.numpy().flatten(),\n",
    "    'CONV1_WEIGHTS': weights_conv1.detach().numpy().flatten(),\n",
    "    'CONV1_BIAS': biases_conv1.detach().numpy().flatten(),\n",
    "    'prec': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_mnist_header_file('mnist_cnn', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 28, 28])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(first_im.shape)\n",
    "net.conv1[0](first_im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 5, 5])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(weights_conv1.shape)\n",
    "print(biases_conv1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_conv1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 28, 28])\n",
      "torch.Size([2, 16, 14, 14])\n",
      "torch.Size([2, 32, 7, 7])\n",
      "torch.Size([2, 1568])\n",
      "tensor([[ 0.0099,  0.0543, -0.0675,  0.1394, -0.0166, -0.0084, -0.0534, -0.1296,\n",
      "          0.0663, -0.0227],\n",
      "        [ 0.0056,  0.0306, -0.0367,  0.1300,  0.0285, -0.0247, -0.1016, -0.1288,\n",
      "          0.0791, -0.0425]], grad_fn=<AddmmBackward0>)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]          12,832\n",
      "              ReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "            Linear-7                   [-1, 10]          15,690\n",
      "================================================================\n",
      "Total params: 28,938\n",
      "Trainable params: 28,938\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.32\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 0.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "summary(net, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05595953,  0.13507354,  0.01593194,  0.00902367, -0.04922011,\n",
       "       -0.18111794, -0.18805149, -0.09560301, -0.10166428,  0.06231072,\n",
       "       -0.05822215, -0.07824443,  0.19068597,  0.06966458,  0.14258046,\n",
       "       -0.09682255], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases_conv1.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 28, 28])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1[0](first_im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05781687, -0.06554271, -0.05869401, -0.06374829, -0.13307634,\n",
       "       -0.08849649, -0.14184302, -0.04242297, -0.25899282, -0.20098922,\n",
       "       -0.1947165 , -0.11197067, -0.15200649, -0.21128216, -0.15689726,\n",
       "       -0.10185251, -0.05595953, -0.05595953], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1[0](first_im)[0][0].detach().numpy()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(first_conv_img + biases_conv1[0].detach().numpy())[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_im[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Feature Map: 1x1x28x28\n",
      "Output Feature Map: 1x16x28x28\n"
     ]
    }
   ],
   "source": [
    "# image dimensions\n",
    "w_in = first_im[0][0].numpy().shape[0]\n",
    "h_in = first_im[0][0].numpy().shape[1]\n",
    "# input channel(s)\n",
    "c_in = 1\n",
    "print(\"Input Feature Map: {}x{}x{}x{}\".format(1, c_in, w_in, h_in))\n",
    "# output channels\n",
    "c_out = 16\n",
    "# kernel size\n",
    "k = 5\n",
    "# stride\n",
    "s = 1\n",
    "# padding\n",
    "p = 2\n",
    "# output dimensions\n",
    "w_out = (w_in - k + 2 * p) // s + 1\n",
    "h_out = (h_in - k + 2 * p) // s + 1\n",
    "print(\"Output Feature Map: {}x{}x{}x{}\".format(1, c_out, w_out, h_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad the image data with zeros\n",
    "# pad_width = ((2,2), (2,2))\n",
    "# first_im[0][0].numpy() = np.pad(first_im[0][0].numpy(), pad_width, 'constant', constant_values=0)\n",
    "# img_padded = np.pad(first_im[0][0].numpy(), ((2,2), (2,2)), 'constant', constant_values=0)\n",
    "# img_padded.shape\n",
    "img_padded = F.pad(first_im, (2, 2, 2, 2))\n",
    "img_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros(shape=(1, c_out, w_out, h_out))\n",
    "for o1 in range(w_out):\n",
    "    for o2 in range(h_out):\n",
    "        for co in range(c_out):\n",
    "            total = 0\n",
    "            for ci in range(c_in):\n",
    "                kt = 0\n",
    "                for kh in range(k):\n",
    "                    for kw in range(k):\n",
    "                        weight = weights_conv1[co][ci][kh][kw].detach().numpy()\n",
    "                        pos1 = kh + o1 * s\n",
    "                        pos2 = kw + o2 * s\n",
    "                        value = img_padded[0][ci][pos1][pos2].numpy()\n",
    "                        kt += weight * value\n",
    "                total += kt\n",
    "            result[0][co][o1][o2] = total + biases_conv1[co].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = np.zeros(shape=(1, c_out, w_out, h_out))\n",
    "for co in range(c_out):\n",
    "    for o1 in range(w_out):\n",
    "        for o2 in range(h_out):\n",
    "            total = 0\n",
    "            for ci in range(c_in):\n",
    "                kt = 0\n",
    "                for kh in range(k):\n",
    "                    for kw in range(k):\n",
    "                        weight = weights_conv1[co][ci][kh][kw].detach().numpy()\n",
    "                        pos1 = kh + o1 * s\n",
    "                        pos2 = kw + o2 * s\n",
    "                        value = img_padded[0][ci][pos1][pos2].numpy()\n",
    "                        kt += weight * value\n",
    "                total += kt\n",
    "            result2[0][co][o1][o2] = total + biases_conv1[co].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.16600159, dtype=float32)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_conv1[0][0][0][1].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 28, 28)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 28, 28)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_padded[0][0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 28, 28])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1[0](first_im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05781688, -0.06554271, -0.05869401, -0.06374828, -0.13307635,\n",
       "       -0.08849649, -0.14184302, -0.04242296, -0.25899283, -0.20098922,\n",
       "       -0.19471649, -0.11197067, -0.15200648, -0.21128216, -0.15689726,\n",
       "       -0.10185252, -0.05595953, -0.05595953])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05781688, -0.06554271, -0.05869401, -0.06374828, -0.13307635,\n",
       "       -0.08849649, -0.14184302, -0.04242296, -0.25899283, -0.20098922,\n",
       "       -0.19471649, -0.11197067, -0.15200648, -0.21128216, -0.15689726,\n",
       "       -0.10185252, -0.05595953, -0.05595953])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2[0][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05595953, -0.05595953, -0.05595953, -0.05595953, -0.05595953,\n",
       "       -0.05781687, -0.06554271, -0.05869401, -0.06374829, -0.13307634,\n",
       "       -0.08849649, -0.14184302, -0.04242297, -0.25899282, -0.20098922,\n",
       "       -0.1947165 , -0.11197067, -0.15200649, -0.21128216, -0.15689726,\n",
       "       -0.10185251, -0.05595953, -0.05595953], dtype=float32)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1[0](first_im)[0][0].detach().numpy()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1529,  0.1660, -0.0469,  0.1837, -0.0438],\n",
       "         [ 0.0404, -0.0974,  0.1175,  0.1763, -0.1467],\n",
       "         [ 0.1738,  0.0374,  0.1478,  0.0271,  0.0964],\n",
       "         [-0.0282,  0.1542,  0.0296, -0.0934,  0.0510],\n",
       "         [-0.0921, -0.0235, -0.0812,  0.1327, -0.1579]]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_conv1[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f2d020fb0ab3aa1fc34292ba06ea4aad36f6d83100c7410d044bd1aea761e0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('msc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
