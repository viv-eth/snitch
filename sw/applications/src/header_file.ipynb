{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2022 ETH Zurich and University of Bologna.\n",
    "# Licensed under the Apache License, Version 2.0, see LICENSE for details.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import pathlib\n",
    "import hjson\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "global verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_cstr(a):\n",
    "    out = '{'\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = a.flat\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        a = a.numpy().flat\n",
    "    for el in a:\n",
    "        out += '{}, '.format(el)\n",
    "    out = out[:-2] + '}'\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_mnist_data(name='mnist', **kwargs):\n",
    "    \n",
    "    # constants\n",
    "    IN_CH1 = kwargs['IN_CH1']\n",
    "    IN_CH2 = kwargs['IN_CH2']\n",
    "    OUT_CH = kwargs['OUT_CH']\n",
    "    DATASET_SIZE = kwargs['DATASET_SIZE']\n",
    "    \n",
    "    # data\n",
    "    # MAT_INPUT = kwargs['INPUT']\n",
    "    # MAT_LABELS = kwargs['LABELS']\n",
    "\n",
    "    # network init parameters from golden model\n",
    "    MAT_WEIGHTS = kwargs['WEIGHTS']\n",
    "    MAT_BIASES = kwargs['BIASES']\n",
    "\n",
    "    IN_CH = IN_CH1*IN_CH2\n",
    "    \n",
    "    layer_str = ''\n",
    "    layer_str += '#include \"network.h\"\\n\\n'\n",
    "    layer_str += f'network_fp64_t {name}_t = {{\\n'\n",
    "    layer_str += f'\\t.IN_CH1 = {IN_CH1},\\n'\n",
    "    layer_str += f'\\t.IN_CH2 = {IN_CH2},\\n'\n",
    "    layer_str += f'\\t.OUT_CH = {OUT_CH},\\n'\n",
    "    layer_str += f'\\t.dtype = FP{kwargs[\"prec\"]}\\n'\n",
    "    layer_str += '};\\n\\n\\n'\n",
    "\n",
    "    ctypes = {\n",
    "        '64': 'double',\n",
    "        '32': 'float',\n",
    "        '16': '__fp16',\n",
    "        'B16': '__bf16',\n",
    "        '8': 'char'\n",
    "    }\n",
    "\n",
    "    dtype = ctypes[str(kwargs['prec'])]\n",
    "\n",
    "    # network initialization\n",
    "    layer_str += f'static {dtype} {name}_weights_dram [{OUT_CH}][{IN_CH}] = ' + array_to_cstr(MAT_WEIGHTS) + ';\\n\\n\\n'\n",
    "    layer_str += f'static {dtype} {name}_biases_dram [{OUT_CH}][{1}] = ' + array_to_cstr(MAT_BIASES) + ';\\n\\n\\n'\n",
    "    # layer_str += f'static {dtype} {name}_weight_grads_dram [{OUT_CH}][{IN_CH}] = ' + array_to_cstr(MAT_WEIGHT_GRADS) + ';\\n\\n\\n'\n",
    "    # layer_str += f'static {dtype} {name}_bias_grads_dram [{OUT_CH}][{1}] = ' + array_to_cstr(MAT_BIAS_GRADS) + ';\\n\\n\\n'\n",
    "\n",
    "\n",
    "    # input data\n",
    "    # layer_str += f'static {dtype} {name}_images_dram [{DATASET_SIZE*IN_CH}][{1}] = ' + array_to_cstr(MAT_INPUT) + ';\\n\\n\\n'\n",
    "    # layer_str += f'static uint32_t {name}_labels_dram [{DATASET_SIZE}][{1}] = ' + array_to_cstr(MAT_LABELS) + ';\\n\\n\\n'\n",
    "    #layer_str += f'static {dtype} {name}_images_dram [{IN_CH}][{1}] = ' + array_to_cstr(MAT_INPUT) + ';\\n\\n\\n'\n",
    "    #layer_str += f'static uint32_t {name}_labels_dram[{1}] = ' + array_to_cstr(MAT_LABELS) + ';\\n\\n\\n'\n",
    "\n",
    "    return layer_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_mnist_header_file(layer_type: str, data_type: str, **kwargs):\n",
    "\n",
    "    file_path = '/scratch1/msc22f11/snitch/sw/applications/data/'\n",
    "    emit_str = \"// Copyright 2022 ETH Zurich and University of Bologna.\\n\" + \\\n",
    "               \"// Licensed under the Apache License, Version 2.0, see LICENSE for details.\\n\" + \\\n",
    "               \"// SPDX-License-Identifier: Apache-2.0\\n\\n\"\n",
    "\n",
    "    if(layer_type == 'mnist'):\n",
    "        if(data_type == 'FP64'):\n",
    "            file = file_path + 'data_fp64_mnist.h'\n",
    "            emit_str += emit_mnist_data(**kwargs)\n",
    "        elif(data_type == 'FP32'):\n",
    "            file = file_path + 'data_fp32_mnist.h'\n",
    "            emit_str += emit_mnist_data(**kwargs)\n",
    "        elif(data_type == 'FP16'):\n",
    "            file = file_path + 'data_fp16_test_mnist.h'\n",
    "            emit_str += emit_mnist_data(**kwargs)\n",
    "        elif(data_type == 'BF16'):\n",
    "            file = file_path + 'data_bf16_mnist.h'\n",
    "            emit_str += emit_mnist_data(**kwargs)\n",
    "        elif(data_type == 'FP8'):\n",
    "            file = file_path + 'data_fp8_test_mnist.h'\n",
    "            emit_str += emit_mnist_data(**kwargs)\n",
    "\n",
    "    #  # if file does not exist create it\n",
    "    # if(not os.path.exists(file)):\n",
    "    #     pathlib.Path(file).touch()\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(emit_str)\n",
    "\n",
    "    print(f'Wrote {file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30.9%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "82.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download MNIST dataset using DataLoader\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "mnist_dataset = MNIST(PATH_DATASETS, train=True, transform=transform, download=True)\n",
    "\n",
    "# set seeds for reproducability \n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "mnist_dl = DataLoader(mnist_dataset, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch = 28*28\n",
    "out_ch = 10\n",
    "\n",
    "class LinLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinLayer, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.lin = nn.Linear(in_ch, out_ch, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        torch.manual_seed(42)\n",
    "        out = self.lin(x.view(x.size(0), -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights shape: torch.Size([10, 784])\n",
      "initial biases shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# import optimizers\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n",
    "\n",
    "net = LinLayer()\n",
    "weights = net.lin.weight\n",
    "print(\"initial weights shape: {}\".format(weights.shape))\n",
    "biases = net.lin.bias\n",
    "print(\"initial biases shape: {}\".format(biases.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special handling for FP8 type where we set up the conversion first\n",
    "import pathlib\n",
    "import ctypes\n",
    "\n",
    "from ctypes import c_uint8, c_double, c_float\n",
    "from ctypes import byref, Structure\n",
    "\n",
    "\n",
    "class flexfloat_desc_t(Structure):\n",
    "    _fields_ = [(\"exp_bits\", c_uint8), (\"frac_bits\", c_uint8)]\n",
    "\n",
    "\n",
    "class flexfloat_t(Structure):\n",
    "    _fields_ = [(\"value\", c_double), (\"desc\", flexfloat_desc_t)]\n",
    "\n",
    "\n",
    "fp64_desc = flexfloat_desc_t(11, 52)\n",
    "fp32_desc = flexfloat_desc_t(8, 23)\n",
    "fp16_desc = flexfloat_desc_t(5, 11)\n",
    "fp16alt_desc = flexfloat_desc_t(8, 7)\n",
    "fp8_desc = flexfloat_desc_t(5, 2)\n",
    "fp8alt_desc = flexfloat_desc_t(4, 3)\n",
    "\n",
    "lib_path = \"/usr/scratch/badile31/msc22f11/msc22f11/PlayGround/flexfloat/src/libflexfloat.so\"\n",
    "ff_lib = ctypes.CDLL(lib_path)\n",
    "\n",
    "ff_get_float = ff_lib.ff_get_float\n",
    "ff_get_float.restype = c_float\n",
    "\n",
    "\n",
    "class ff:\n",
    "    def __init__(self, value: float, desc: flexfloat_desc_t = fp64_desc):\n",
    "        self.desc = desc\n",
    "        self.value = value\n",
    "        self.a = flexfloat_t(value, desc)\n",
    "        ff_lib.ff_init_float(byref(self.a), c_float(value), desc)\n",
    "\n",
    "    def __add__(self, b):\n",
    "        ff_res = flexfloat_t(0.0, self.desc)\n",
    "        ff_lib.ff_add(byref(ff_res), byref(self.a), byref(b.a))\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert float32 to binary representation\n",
    "import struct\n",
    "\n",
    "def float32_to_bin(value):\n",
    "    return ''.join(f'{c:0>8b}' for c in struct.pack('!f', value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We have to handle denormalized numbers:\n",
    "    \n",
    "    +INF will be represented in FP8 as 0 11111 00 \n",
    "    -INF will be represented in FP8 as 1 11111 00\n",
    "    +0 will be represented in FP8 as 0 00000 00\n",
    "    -0 will be represented in FP8 as 1 00000 00\n",
    "    NaN will be represented in FP8 as X 11111 MM (at least one of the MMM bits is set, sign bit is don't care)\n",
    "\n",
    "According to https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8556098 denormalized transprecision numbers\n",
    "will be represented by their high precision counterparts. In these cases we have to make sure that we do not adjust\n",
    "the exponent. In the other cases we adjust the exponent and cut the mantissa.\n",
    "\"\"\"\n",
    "\n",
    "from numpy import binary_repr\n",
    "\n",
    "# this function returns an 8 character string representing the binary representation of the FP8 number\n",
    "def float32_to_fp8(value):\n",
    "    max_exp_fp32 = int('11111111', 2)\n",
    "    min_exp_fp32 = int('00000000', 2)\n",
    "    exp_bias_fp32 = 2 ** (8 - 1) - 1\n",
    "    exp_bias_fp8 = 2 ** (5 - 1) - 1\n",
    "    # get the binary representation of the number\n",
    "    binstr = float32_to_bin(value)\n",
    "    # extract sign, exponent and mantissa bits\n",
    "    sign = binstr[0]\n",
    "    exponent = binstr[1:9]\n",
    "    mantissa = binstr[9:]\n",
    "    # check if the number is denormalized\n",
    "    # we start by checking if all exponent bits are asserted\n",
    "    if(int(exponent) == max_exp_fp32):\n",
    "        # if so, we check if the mantissa is all zeros (will result in +/-INF)\n",
    "        if(int(mantissa) == 0):\n",
    "            return '0b' + sign + exponent[:5] + mantissa[:2]\n",
    "        # if not, we have to return a NaN\n",
    "        else:\n",
    "            return '0b' + sign + exponent[:5] + '01'\n",
    "    # if both exponent and mantissa are zero we will return +/-0\n",
    "    elif (int(exponent) == min_exp_fp32 and int(mantissa) == 0): \n",
    "        return '0b' + sign + exponent[:5] + mantissa[:2]\n",
    "    else :\n",
    "        # if not, we adjust the exponent and cut the mantissa\n",
    "        exponent_fp8 = binary_repr(int(exponent, 2) - exp_bias_fp32 + exp_bias_fp8, width=5)\n",
    "        mantissa_fp8 = mantissa[:2]\n",
    "        return '0b' + sign + exponent_fp8 + mantissa_fp8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponent is stored in two's complement\n",
    "def twos_comp(val, bits):\n",
    "    \"\"\"compute the 2's complement of int value val\"\"\"\n",
    "    if (val & (1 << (bits - 1))) != 0: # if sign bit is set e.g., 8bit: 128-255\n",
    "        val = val - (1 << bits)        # compute negative value\n",
    "    return val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_fp8_decimal(binstr):\n",
    "\n",
    "    # extract sign, exponent and mantissa bits\n",
    "    sign = binstr[0]\n",
    "    num_sign_bits = 1\n",
    "    # print(f'Sign:     ({num_sign_bits} bit)  = {sign}')\n",
    "    exponent = binstr[1:6]\n",
    "    num_exp_bits = len(exponent)\n",
    "    # print(f'Exponent: ({num_exp_bits} bit)  = {exponent}')\n",
    "    mantissa = binstr[6:]\n",
    "    num_mant_bits = len(mantissa)\n",
    "    # print(f'Mantissa: ({num_mant_bits} bit) = {mantissa}')\n",
    "\n",
    "    exp_bias_fp8 = 2 ** (5 - 1) - 1\n",
    "    dec_val_fp8 = (-1)**(int(sign, 2)) * (1 + (int(mantissa, 2))/(2**num_mant_bits)) * 2**(twos_comp(int(exponent, 2), num_exp_bits) - exp_bias_fp8)\n",
    "    if(int(sign, 2) == 0 and int(exponent, 2) == 0 and int(mantissa, 2) == 0):\n",
    "        dec_val_fp8 = 0\n",
    "    # print(\"\\nBinary to floating point number (FP8) conversion using formula: \", dec_val_fp8)\n",
    "    return dec_val_fp8\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NaN: 01111101 = 125\n",
      " NaN: 01111110 = 126\n",
      " NaN: 01111111 = 127\n",
      " NaN: 11111101 = 253\n",
      " NaN: 11111110 = 254\n",
      " NaN: 11111111 = 255\n",
      "+INF: 01111100 = 124\n",
      "-INF: 11111100 = 252\n"
     ]
    }
   ],
   "source": [
    "# define the special FP8 values\n",
    "fp8_nans = ['01111101', '01111110', '01111111', '11111101', '11111110', '11111111']\n",
    "# print fp8_nans as integers\n",
    "for nan in fp8_nans:\n",
    "    print(f' NaN: {nan} = {int(nan, 2)}')\n",
    "fp8_pinf = '01111100'\n",
    "print(f'+INF: {fp8_pinf} = {int(fp8_pinf, 2)}')\n",
    "fp8_ninf = '11111100'\n",
    "print(f'-INF: {fp8_ninf} = {int(fp8_ninf, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp8_int_values = range(0, 256)\n",
    "# remove the special FP8 values\n",
    "fp8_int_values = [x for x in fp8_int_values if x not in [int(nan, 2) for nan in fp8_nans] + [int(fp8_pinf, 2), int(fp8_ninf, 2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def clear_special_values(data_np):\n",
    "    nan_cnt = 0\n",
    "    inf_cnt = 0\n",
    "    for val, idx in zip(data_np, range(len(data_np))):\n",
    "        if(val in [int(nan, 2) for nan in fp8_nans]):\n",
    "            print(f'Found NaN at index {idx}')\n",
    "            data_np[idx] = random.choice(fp8_int_values)\n",
    "            print(f'Randomly replaced NaN with {data_np[idx]}')\n",
    "            nan_cnt += 1\n",
    "        elif(val == int(fp8_pinf, 2) or val == int(fp8_ninf, 2)):\n",
    "            print(f'Found INF at index {idx}')\n",
    "            data_np[idx] = random.choice(fp8_int_values)\n",
    "            print(f'Randomly replaced INF with {data_np[idx]}')\n",
    "            inf_cnt += 1 \n",
    "\n",
    "    print(f'NaN count: {nan_cnt}')\n",
    "    print(f'INF count: {inf_cnt}')\n",
    "\n",
    "    return data_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_target_format(data_np, prec):\n",
    "    if prec == 64:\n",
    "        return data_np.astype(np.float64)\n",
    "    elif prec == 32:\n",
    "        return data_np.astype(np.float32)\n",
    "    elif prec == 16:\n",
    "        return data_np.astype(np.float16)\n",
    "    elif prec == 8:\n",
    "        data_np = data_np.astype(np.float32)\n",
    "        # convert to FP8\n",
    "        data_fp8 = np.array([float32_to_fp8(val) for val in data_np])\n",
    "        # clear special values\n",
    "        data_fp8 = clear_special_values(data_fp8)\n",
    "        return data_fp8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03156039, -0.0312207 , -0.02688671, ...,  0.03123467,\n",
       "        0.00186213,  0.0296352 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data type: float64\n",
      "Original data shape: (7840,)\n",
      "Weights - FP64 data type: float64\n",
      "Weights - FP64 data shape: (7840,)\n",
      "Biases - FP64 data type: float64\n",
      "Biases - FP64 data shape: (10,)\n",
      "Weights - FP32 data type: float32\n",
      "Biases - FP32 data shape: (7840,)\n",
      "Biases - FP32 data type: float32\n",
      "Biases - FP32 data shape: (10,)\n",
      "Weights - FP16 data type: float16\n",
      "Weights - FP16 data shape: (7840,)\n",
      "Biases - FP16 data type: float16\n",
      "Biases - FP16 data shape: (10,)\n",
      "NaN count: 0\n",
      "INF count: 0\n",
      "NaN count: 0\n",
      "INF count: 0\n",
      "Weights - FP8 data type: <U10\n",
      "Weights - FP8 data shape: (7840,)\n",
      "Biases - FP8 data type: <U10\n",
      "Biases - FP8 data shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# print data type and shape\n",
    "print(f'Original data type: {weights.detach().numpy().flatten().dtype}')\n",
    "print(f'Original data shape: {weights.detach().numpy().flatten().shape}')\n",
    "# convert to target format\n",
    "weights_fp64 = convert_to_target_format(weights.detach().numpy().flatten(), 64)\n",
    "biases_fp64 = convert_to_target_format(biases.detach().numpy().flatten(), 64)\n",
    "print(f'Weights - FP64 data type: {weights_fp64.dtype}')\n",
    "print(f'Weights - FP64 data shape: {weights_fp64.shape}')\n",
    "print(f'Biases - FP64 data type: {biases_fp64.dtype}')\n",
    "print(f'Biases - FP64 data shape: {biases_fp64.shape}')\n",
    "weights_fp32 = convert_to_target_format(weights.detach().numpy().flatten(), 32)\n",
    "biases_fp32 = convert_to_target_format(biases.detach().numpy().flatten(), 32)\n",
    "print(f'Weights - FP32 data type: {weights_fp32.dtype}')\n",
    "print(f'Biases - FP32 data shape: {weights_fp32.shape}')\n",
    "print(f'Biases - FP32 data type: {biases_fp32.dtype}')\n",
    "print(f'Biases - FP32 data shape: {biases_fp32.shape}')\n",
    "weights_fp16 = convert_to_target_format(weights.detach().numpy().flatten(), 16)\n",
    "biases_fp16 = convert_to_target_format(biases.detach().numpy().flatten(), 16)\n",
    "print(f'Weights - FP16 data type: {weights_fp16.dtype}')\n",
    "print(f'Weights - FP16 data shape: {weights_fp16.shape}')\n",
    "print(f'Biases - FP16 data type: {biases_fp16.dtype}')\n",
    "print(f'Biases - FP16 data shape: {biases_fp16.shape}')\n",
    "weights_fp8 = convert_to_target_format(weights.detach().numpy().flatten(), 8)\n",
    "biases_fp8 = convert_to_target_format(biases.detach().numpy().flatten(), 8)\n",
    "print(f'Weights - FP8 data type: {weights_fp8.dtype}')\n",
    "print(f'Weights - FP8 data shape: {weights_fp8.shape}')\n",
    "print(f'Biases - FP8 data type: {biases_fp8.dtype}')\n",
    "print(f'Biases - FP8 data shape: {biases_fp8.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "            'IN_CH1': 28,\n",
    "            'IN_CH2': 28,\n",
    "            'OUT_CH': 10,\n",
    "            'DATASET_SIZE': 5,\n",
    "            'WEIGHTS': weights_fp16,\n",
    "            'BIASES': biases_fp16,\n",
    "            'prec': 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /scratch1/msc22f11/snitch/sw/applications/data/data_fp16_test_mnist.h\n"
     ]
    }
   ],
   "source": [
    "emit_mnist_header_file('mnist', 'FP16', **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('msc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ecad0643b88c81fa0e19ea49e906f3e8e46360fe82f2ddcb2e85abcf6121e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
