{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2022 ETH Zurich and University of Bologna.\n",
    "# Licensed under the Apache License, Version 2.0, see LICENSE for details.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import pathlib\n",
    "import hjson\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "global verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "torch.Size([16, 784])\n",
      "torch.Size([16, 784])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "tensor([9])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "# get input channels\n",
    "IN_CH = 1 * 28 * 28 # 1 channels, 32x32 pixels // FP64: 1 x 28 x 28, FP32: 1 x 40 x 40, FP16: 3 x 32 x 32, FP8: 3 * 48 * 48\n",
    "OUT_CH = 16 # 16 classes\n",
    "r1 = 0\n",
    "r2 = 0.5\n",
    "\n",
    "data_type = torch.float64\n",
    "\n",
    "# get random input data with shape (IN_CH, 1)\n",
    "# input = first_im.to(torch.float64).view(first_im.to(torch.float64).size(0), -1) #torch.randn(IN_CH)\n",
    "input = torch.randn(IN_CH, dtype=data_type)\n",
    "print(input.shape)\n",
    "\n",
    "# get random weights with shape (OUT_CH, IN_CH)\n",
    "weights = torch.FloatTensor(OUT_CH, IN_CH).uniform_(r1, r2).to(data_type) #torch.randn(OUT_CH, IN_CH).to(torch.float32)\n",
    "print(weights.shape)\n",
    "\n",
    "# get random weight gradients with shape (OUT_CH, IN_CH)\n",
    "weight_grads = torch.FloatTensor(OUT_CH, IN_CH).uniform_(r1, r2).to(data_type) \n",
    "print(weight_grads.shape)\n",
    "\n",
    "# get random bias with shape (OUT_CH, 1)\n",
    "bias = torch.FloatTensor(OUT_CH).uniform_(r1, r2).to(data_type)#torch.randn(OUT_CH).to(torch.float32)\n",
    "print(bias.shape)\n",
    "\n",
    "# get random bias gradients with shape (OUT_CH, 1)\n",
    "bias_grads = torch.FloatTensor(OUT_CH).uniform_(r1, r2).to(data_type)\n",
    "print(bias_grads.shape)\n",
    "\n",
    "# calculate the activations of the linear layer\n",
    "activations = input @ weights.t() + bias\n",
    "print(activations.shape)\n",
    "# get a random integer between 0 and 16\n",
    "label = torch.randint(0, 16, (1,))\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion helper functions for FP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert float32 to binary representation\n",
    "import struct\n",
    "\n",
    "def float32_to_bin(value):\n",
    "    return ''.join(f'{c:0>8b}' for c in struct.pack('!f', value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We have to handle denormalized numbers:\n",
    "    \n",
    "    +INF will be represented in FP8 as 0 11111 00 \n",
    "    -INF will be represented in FP8 as 1 11111 00\n",
    "    +0 will be represented in FP8 as 0 00000 00\n",
    "    -0 will be represented in FP8 as 1 00000 00\n",
    "    NaN will be represented in FP8 as X 11111 MM (at least one of the MMM bits is set, sign bit is don't care)\n",
    "\n",
    "According to https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8556098 denormalized transprecision numbers\n",
    "will be represented by their high precision counterparts. In these cases we have to make sure that we do not adjust\n",
    "the exponent. In the other cases we adjust the exponent and cut the mantissa.\n",
    "\"\"\"\n",
    "\n",
    "from numpy import binary_repr\n",
    "\n",
    "# this function returns an 8 character string representing the binary representation of the FP8 number\n",
    "def float32_to_fp8(value):\n",
    "    max_exp_fp32 = int('11111111', 2)\n",
    "    min_exp_fp32 = int('00000000', 2)\n",
    "    exp_bias_fp32 = 2 ** (8 - 1) - 1\n",
    "    exp_bias_fp8 = 2 ** (5 - 1) - 1\n",
    "    # get the binary representation of the number\n",
    "    binstr = float32_to_bin(value)\n",
    "    # extract sign, exponent and mantissa bits\n",
    "    sign = binstr[0]\n",
    "    exponent = binstr[1:9]\n",
    "    mantissa = binstr[9:]\n",
    "    # check if the number is denormalized\n",
    "    # we start by checking if all exponent bits are asserted\n",
    "    if(int(exponent) == max_exp_fp32):\n",
    "        # if so, we check if the mantissa is all zeros (will result in +/-INF)\n",
    "        if(int(mantissa) == 0):\n",
    "            return '0b' + sign + exponent[:5] + mantissa[:2]\n",
    "        # if not, we have to return a NaN\n",
    "        else:\n",
    "            return '0b' + sign + exponent[:5] + '01'\n",
    "    # if both exponent and mantissa are zero we will return +/-0\n",
    "    elif (int(exponent) == min_exp_fp32 and int(mantissa) == 0): \n",
    "        return '0b' + sign + exponent[:5] + mantissa[:2]\n",
    "    else :\n",
    "        # if not, we adjust the exponent and cut the mantissa\n",
    "        exponent_fp8 = binary_repr(int(exponent, 2) - exp_bias_fp32 + exp_bias_fp8, width=5)\n",
    "        mantissa_fp8 = mantissa[:2]\n",
    "        return '0b' + sign + exponent_fp8 + mantissa_fp8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponent is stored in two's complement\n",
    "def twos_comp(val, bits):\n",
    "    \"\"\"compute the 2's complement of int value val\"\"\"\n",
    "    if (val & (1 << (bits - 1))) != 0: # if sign bit is set e.g., 8bit: 128-255\n",
    "        val = val - (1 << bits)        # compute negative value\n",
    "    return val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_fp8_decimal(binstr):\n",
    "\n",
    "    # extract sign, exponent and mantissa bits\n",
    "    sign = binstr[0]\n",
    "    num_sign_bits = 1\n",
    "    # print(f'Sign:     ({num_sign_bits} bit)  = {sign}')\n",
    "    exponent = binstr[1:6]\n",
    "    num_exp_bits = len(exponent)\n",
    "    # print(f'Exponent: ({num_exp_bits} bit)  = {exponent}')\n",
    "    mantissa = binstr[6:]\n",
    "    num_mant_bits = len(mantissa)\n",
    "    # print(f'Mantissa: ({num_mant_bits} bit) = {mantissa}')\n",
    "\n",
    "    exp_bias_fp8 = 2 ** (5 - 1) - 1\n",
    "    dec_val_fp8 = (-1)**(int(sign, 2)) * (1 + (int(mantissa, 2))/(2**num_mant_bits)) * 2**(twos_comp(int(exponent, 2), num_exp_bits) - exp_bias_fp8)\n",
    "    if(int(sign, 2) == 0 and int(exponent, 2) == 0 and int(mantissa, 2) == 0):\n",
    "        dec_val_fp8 = 0\n",
    "    # print(\"\\nBinary to floating point number (FP8) conversion using formula: \", dec_val_fp8)\n",
    "    return dec_val_fp8\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for FP8 conversion\n",
    "# convert weights to FP8\n",
    "weights = [float32_to_fp8(x) for x in weights.flatten().tolist()]\n",
    "# convert bias to FP8\n",
    "bias = [float32_to_fp8(x) for x in bias.flatten().tolist()]\n",
    "# convert input to FP8\n",
    "input = [float32_to_fp8(x) for x in input.flatten().tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the memory requirements for given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP64 memory reguirements:\n",
      "Input size: 6.125 KB\n",
      "Weights size: 98.0 KB\n",
      "Bias size: 0.125 KB\n",
      "Output size: 0.125 KB\n",
      "\n",
      "Total size: 104.25 KB\n"
     ]
    }
   ],
   "source": [
    "# calculate the memory requirements\n",
    "if(torch.is_tensor(bias[0]) and bias[0].dtype == torch.float64):\n",
    "    print(\"FP64 memory reguirements:\")\n",
    "    print(f'Input size: {IN_CH * 64 / 8 / 1024} KB')\n",
    "    print(f'Weights size: {OUT_CH * IN_CH * 64 / 8 / 1024} KB')\n",
    "    print(f'Bias size: {OUT_CH * 64 / 8 / 1024} KB')\n",
    "    print(f'Output size: {OUT_CH * 64 / 8 / 1024} KB')\n",
    "    print(f'\\nTotal size: {(IN_CH + OUT_CH * IN_CH + OUT_CH) * 64 / 8 / 1024} KB')\n",
    "elif(torch.is_tensor(bias[0]) and bias[0].dtype == torch.float32):\n",
    "    print(\"FP32 memory reguirements:\")\n",
    "    print(f'Input size: {IN_CH * 32 / 8 / 1024} KB')\n",
    "    print(f'Weights size: {OUT_CH * IN_CH * 32 / 8 / 1024} KB')\n",
    "    print(f'Bias size: {OUT_CH * 32 / 8 / 1024} KB')\n",
    "    print(f'Output size: {OUT_CH * 32 / 8 / 1024} KB')\n",
    "    print(f'\\nTotal size: {(IN_CH + OUT_CH * IN_CH + OUT_CH) * 32 / 8 / 1024} KB')\n",
    "elif(torch.is_tensor(bias[0]) and bias[0].dtype == torch.float16):\n",
    "    print(\"FP16 memory reguirements:\")\n",
    "    print(f'Input size: {IN_CH * 16 / 8 / 1024} KB')\n",
    "    print(f'Weights size: {OUT_CH * IN_CH * 16 / 8 / 1024} KB')\n",
    "    print(f'Bias size: {OUT_CH * 16 / 8 / 1024} KB')\n",
    "    print(f'Output size: {OUT_CH * 16 / 8 / 1024} KB')\n",
    "    print(f'\\nTotal size: {(IN_CH + OUT_CH * IN_CH + OUT_CH) * 16 / 8 / 1024} KB')\n",
    "else:\n",
    "    print(\"FP8 memory reguirements:\")\n",
    "    print(f'Input size: {IN_CH * 8 / 8 / 1024} KB')\n",
    "    print(f'Weights size: {OUT_CH * IN_CH * 8 / 8 / 1024} KB')\n",
    "    print(f'Bias size: {OUT_CH * 8 / 8 / 1024} KB')\n",
    "    print(f'Output size: {OUT_CH * 8 / 8 / 1024} KB')\n",
    "    print(f'\\nTotal size: {(IN_CH + OUT_CH * IN_CH + OUT_CH) * 8 / 8 / 1024} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_cstr(a):\n",
    "    out = '{'\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = a.flat\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        a = a.numpy().flat\n",
    "    for el in a:\n",
    "        out += '{}, '.format(el)\n",
    "    out = out[:-2] + '}'\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_mnist_data(name='mnist', **kwargs):\n",
    "    \n",
    "    # constants\n",
    "    IN_CH = kwargs['IN_CH']\n",
    "    OUT_CH = kwargs['OUT_CH']\n",
    "    DATASET_SIZE = kwargs['DATASET_SIZE']\n",
    "    \n",
    "    # data\n",
    "    MAT_INPUT = kwargs['INPUT']\n",
    "    MAT_LABELS = kwargs['LABELS']\n",
    "\n",
    "    # network init parameters from golden model\n",
    "    MAT_WEIGHTS = kwargs['WEIGHTS']\n",
    "    MAT_WEIGHT_GRADS = kwargs['WEIGHT_GRADS']\n",
    "    MAT_BIASES = kwargs['BIASES'] \n",
    "    MAT_BIAS_GRADS = kwargs['BIAS_GRADS']\n",
    "    \n",
    "    layer_str = ''\n",
    "    layer_str += '#include \"network.h\"\\n\\n'\n",
    "    layer_str += f'network_single_cluster_t {name}_t = {{\\n'\n",
    "    layer_str += f'\\t.IN_CH = {IN_CH},\\n'\n",
    "    layer_str += f'\\t.OUT_CH = {OUT_CH},\\n'\n",
    "    layer_str += f'\\t.dtype = FP{kwargs[\"prec\"]}\\n'\n",
    "    layer_str += '};\\n\\n\\n'\n",
    "\n",
    "    ctypes = {\n",
    "        '64': 'double',\n",
    "        '32': 'float',\n",
    "        '16': '__fp16',\n",
    "        'B16': '__bf16',\n",
    "        '8': 'char'\n",
    "    }\n",
    "\n",
    "    dtype = ctypes[str(kwargs['prec'])]\n",
    "\n",
    "    # network initialization\n",
    "    layer_str += f'static {dtype} {name}_weights_dram [{OUT_CH}][{IN_CH}] = ' + array_to_cstr(MAT_WEIGHTS) + ';\\n\\n\\n'\n",
    "    layer_str += f'static {dtype} {name}_weight_grads_dram [{OUT_CH}][{IN_CH}] = ' + array_to_cstr(MAT_WEIGHT_GRADS) + ';\\n\\n\\n'\n",
    "    layer_str += f'static {dtype} {name}_biases_dram [{OUT_CH}][{1}] = ' + array_to_cstr(MAT_BIASES) + ';\\n\\n\\n'\n",
    "    layer_str += f'static {dtype} {name}_bias_grads_dram [{OUT_CH}][{1}] = ' + array_to_cstr(MAT_BIAS_GRADS) + ';\\n\\n\\n'\n",
    "\n",
    "\n",
    "    # input data\n",
    "    layer_str += f'static {dtype} {name}_images_dram [{DATASET_SIZE*IN_CH}][{1}] = ' + array_to_cstr(MAT_INPUT) + ';\\n\\n\\n'\n",
    "    layer_str += f'static uint32_t {name}_labels_dram [{DATASET_SIZE}][{1}] = ' + array_to_cstr(MAT_LABELS) + ';\\n\\n\\n'\n",
    "\n",
    "    return layer_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_mnist_header_file(layer_type: str, **kwargs):\n",
    "\n",
    "    file_path = '/scratch/msc22f11/msc22f11/snitch/sw/applications/data/'\n",
    "    emit_str = \"// Copyright 2022 ETH Zurich and University of Bologna.\\n\" + \\\n",
    "               \"// Licensed under the Apache License, Version 2.0, see LICENSE for details.\\n\" + \\\n",
    "               \"// SPDX-License-Identifier: Apache-2.0\\n\\n\"\n",
    "\n",
    "    if(layer_type == 'mnist'):\n",
    "        file = file_path + 'data_fp64_all_mnist.h'\n",
    "        emit_str += emit_mnist_data(**kwargs)\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(emit_str)\n",
    "\n",
    "    print(\"File written to: \" + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "            'IN_CH': IN_CH,\n",
    "            'OUT_CH': OUT_CH,\n",
    "            'DATASET_SIZE': 1,\n",
    "            'INPUT': input, #input.to(torch.float16),\n",
    "            'WEIGHTS': weights, #weights.detach().to(torch.float16),\n",
    "            'WEIGHT_GRADS': weight_grads,\n",
    "            'BIASES': bias,#bias.detach().to(torch.float16),\n",
    "            'BIAS_GRADS': bias_grads,\n",
    "            'LABELS': label,\n",
    "            'prec': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written to: /scratch/msc22f11/msc22f11/snitch/sw/applications/data/data_fp64_all_mnist.h\n"
     ]
    }
   ],
   "source": [
    "emit_mnist_header_file('mnist', **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('msc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ecad0643b88c81fa0e19ea49e906f3e8e46360fe82f2ddcb2e85abcf6121e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
